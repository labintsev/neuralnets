"""
DO NOT EDIT THIS FILE !!!
"""
import unittest
from seminar4 import *
from test_utils import check_layer_gradient, get_preprocessed_data


class TestSgd(unittest.TestCase):
    """1 point"""

    def setUp(self) -> None:
        self.w = np.zeros((32, 10))
        self.d_w = np.ones((32, 10))
        self.lr = 1e-3

    def testStep1(self):
        optimizer = SGD()
        optimizer.step(self.w, self.d_w, self.lr)
        self.assertTrue(np.array_equal(- self.d_w * self.lr, self.w))

    def testStep2(self):
        optimizer = SGD()
        optimizer.step(self.w, self.d_w, self.lr)
        optimizer.step(self.w, self.d_w, self.lr)
        self.assertTrue(np.array_equal(- 2 * self.d_w * self.lr, self.w))


class TestMomentum(unittest.TestCase):
    """1 point"""

    def setUp(self) -> None:
        self.w = np.zeros((32, 10))
        self.d_w = np.ones((32, 10))
        self.lr = 1e-3

    def testStep1(self):
        optimizer = Momentum(rho=0.5)
        optimizer.step(self.w, self.d_w, self.lr)
        self.assertTrue(np.array_equal(- self.d_w * self.lr, self.w))
        self.assertTrue(np.array_equal(self.d_w, optimizer.velocity))

    def testStep2(self):
        optimizer = Momentum(rho=0.5)
        optimizer.step(self.w, self.d_w, self.lr)
        optimizer.step(self.w, self.d_w, self.lr)
        self.assertTrue(np.array_equal(- 2.5 * self.d_w * self.lr, self.w))
        self.assertTrue(np.array_equal(1.5 * self.d_w, optimizer.velocity))


class TestDropout(unittest.TestCase):
    """1 point"""

    def testForward(self):
        x = np.ones((1, 10))
        dropout = DropoutLayer(0.5)
        z = dropout.forward(x)
        self.assertEqual(dropout.scale, 1 / (1 - dropout.p))
        inverses = (1 - dropout.mask) * z
        self.assertEqual(inverses.sum(), 0.0)

    def testBackward(self):
        x = np.ones((128, 10))
        dropout = DropoutLayer()
        z = dropout.forward(x)
        dz = np.ones_like(z)
        dx = dropout.backward(dz)
        self.assertAlmostEqual(dz.mean(), dx.mean(), places=1)
        inverses = (1 - dropout.mask) * dx
        self.assertEqual(inverses.sum(), 0.0)


class TestBatchNormalization(unittest.TestCase):
    """1 point"""

    def setUp(self) -> None:
        self.bs = 2
        self.dims = 10

    def testForwardOnes(self):
        x = np.ones((self.bs, self.dims))
        layer = BatchNormLayer(self.dims)
        z = layer.forward(x)
        self.assertTrue(np.array_equal(np.zeros((self.bs, self.dims)), z))

    def testForwardTrain(self):
        x = np.random.randint(0, 2, (32, 10))
        layer = BatchNormLayer(self.dims)
        z = layer.forward(x)
        z_mean = np.mean(z)
        z_std = np.std(z)
        self.assertAlmostEqual(float(z_mean), 0.0, places=2)
        self.assertAlmostEqual(float(z_std), 1.0, places=2)

    def testForwardNotTrain(self):
        x = np.random.randint(0, 2, (32, 10))
        layer = BatchNormLayer(self.dims)
        layer.forward(x, train=True)
        z = layer.forward(x * 10, train=False)
        z_mean = np.mean(z)
        z_std = np.std(z)
        self.assertLess(z_mean - 9.0, 1.0)
        self.assertLess(z_std - 10.0, 1.0)

    def testBackward(self):
        x = np.random.randint(0, 2, (2, 10))
        batchNormLayer = BatchNormLayer(10)
        result = check_layer_gradient(batchNormLayer, x)
        self.assertTrue(result)


class TestOverFitting(unittest.TestCase):
    """1 point"""
    def setUp(self) -> None:
        self.n_samples = 128
        (x_train, y_train), _ = get_preprocessed_data(include_bias=False)
        dev_idx = np.random.choice(len(x_train), self.n_samples)
        self.X_dev, self.y_dev = x_train[dev_idx], y_train[dev_idx]

    def testOverFittingSGD(self):
        print('Test SGD')
        neural_net = NeuralNetwork([DenseLayer(3072, 256),
                                    DropoutLayer(0.5),
                                    BatchNormLayer(256),
                                    ReLULayer(),
                                    DenseLayer(256, 10)])
        neural_net.setup_optimizer(SGD())
        loss_history = neural_net.fit(self.X_dev, self.y_dev,
                                      learning_rate=5e-3, num_iters=300,
                                      batch_size=self.n_samples, verbose=True)
        self.assertLess(loss_history[-1], 1.0)
        self.assertGreater(loss_history[-1], 0.0)

    def testOverFittingMomentum(self):
        print('Test Momentum')
        neural_net = NeuralNetwork([DenseLayer(3072, 256),
                                    DropoutLayer(0.5),
                                    BatchNormLayer(256),
                                    ReLULayer(),
                                    DenseLayer(256, 10)])
        neural_net.setup_optimizer(Momentum())
        loss_history = neural_net.fit(self.X_dev, self.y_dev,
                                      learning_rate=5e-3, num_iters=300,
                                      batch_size=self.n_samples, verbose=True)
        self.assertLess(loss_history[-1], 1.0)
        self.assertGreater(loss_history[-1], 0.0)


if __name__ == '__main__':
    unittest.main()
